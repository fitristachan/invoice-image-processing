{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport io\nimport json\nimport tensorflow as tf\nfrom PIL import Image\nfrom tqdm import tqdm\nimport numpy as np\nfrom datasets import load_dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = load_dataset(\"naver-clova-ix/cord-v2\", split=\"train\")\nval_dataset = load_dataset(\"naver-clova-ix/cord-v2\", split=\"validation\")\n\n# example = train_dataset[0][\"ground_truth\"]\n# example_json = json.loads(example)\n# boxes=[]\n\n# for line in example_json.get('valid_line', []):\n#     category = line.get('category', '')\n                \n#     for word in line.get('words', []):\n#         if 'quad' not in word:\n#             continue\n                    \n#         x_coords = [float(word['quad'][f'x{i}']) for i in range(1, 5)]\n#         y_coords = [float(word['quad'][f'y{i}']) for i in range(1, 5)]\n             \n#         print (\"x_coords\", x_coords)\n#         print (\"y_coords\", y_coords)\n#         boxes.append([\n#             min(x_coords),\n#             min(y_coords),\n#             max(x_coords),\n#             max(y_coords)\n#         ])\n#         print(boxes)\n#         print(len(boxes))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"category_map = {\n    \"menu.nm\": 0,\n    \"menu.cnt\": 1,\n    \"menu.price\": 2,\n    \"background\": 3  # Untuk anchor yang tidak match dengan GT\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"STEPS_PER_EPOCH = 1000\nEPOCH = 5\nINITIAL_LR = 1e-4\nMOMENTUM = 0.9\nGRADIENT_CLIP_NORM = 1.0\nSTD_DEV = 0.01","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.applications import ResNet50V2\nfrom keras import layers, Model\nfrom keras.layers import Input, Conv2D, Concatenate, UpSampling2D, Dense, Reshape\n\n# Backbone model\npre_trained_model = ResNet50V2(input_shape=(640, 640, 3),\n                              include_top=False,\n                              weights='imagenet')\n\n# Get feature maps at different scales\nc3_output = pre_trained_model.get_layer(\"conv3_block4_out\").output  # 75x75\nc4_output = pre_trained_model.get_layer(\"conv4_block6_out\").output  # 38x38\nc5_output = pre_trained_model.get_layer(\"conv5_block3_out\").output  # 19x19\n\n# Build FPN (Feature Pyramid Network)\n# Top-down pathway\np5 = Conv2D(64, 1, padding='same', activation='relu')(c5_output)  # stride 32\np5_upsampled = UpSampling2D(size=(1, 1), interpolation='bilinear')(p5) \n\np4 = Conv2D(64, 1, padding='same', activation='relu')(c4_output)  # stride 16\np4 = Concatenate()([p5_upsampled, p4])\np4 = Conv2D(64, 3, padding='same', activation='relu')(p4)\np4_upsampled = UpSampling2D(size=(2, 2), interpolation='bilinear')(p4)  # 32x32 → 64x64\n\np3 = Conv2D(64, 1, padding='same', activation='relu')(c3_output)  # stride 8\np3 = Concatenate()([p4_upsampled, p3])\np3 = Conv2D(64, 3, padding='same', activation='relu')(p3)\n\n# Final feature map for detection\nfeatures = p3\n\n70800\n# Calculate total anchors based on ACTUAL feature map size\nheight, width = 40, 40  # From your error message\nnum_anchors_per_location = 9  # 3 scales × 3 ratios\ntotal_anchors = height * width * num_anchors_per_location  # 40×40×9 = 14400\n\n# Classification Head\ncls_head = Conv2D(64, 3, padding='same', activation='relu')(features)\ncls_head = Conv2D(num_anchors_per_location * 4, 3, padding='same')(cls_head)  # 4 classes\ncls_output = Reshape((total_anchors, 4))(cls_head)  # (None, 14400, 4)\ncls_output = layers.Activation('softmax', dtype='float32', name='class_output')(cls_output)\n\n# Regression Head\nreg_head = Conv2D(64, 3, padding='same', activation='relu')(features)\nreg_head = Conv2D(num_anchors_per_location * 4, 3, padding='same')(reg_head)\nbbox_output = Reshape((total_anchors, 4))(reg_head)  # (None, 14400, 4)\nbbox_output = layers.Activation('sigmoid', dtype='float32', name='bbox_output')(bbox_output)\n\nmodel = Model(inputs=pre_trained_model.input, outputs=[cls_output, bbox_output])\nmodel.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\ndef generate_anchors(base_size, ratios, scales):\n    \"\"\"Generate base anchors dengan format [x1, y1, x2, y2]\"\"\"\n    anchors = []\n    for scale in scales:\n        for ratio in ratios:\n            w = base_size * scale * np.sqrt(ratio)\n            h = base_size * scale / np.sqrt(ratio)\n            # Format [x1, y1, x2, y2] relatif terhadap center (0,0)\n            anchors.append([-w/2, -h/2, w/2, h/2])\n    return np.array(anchors)\n\ndef generate_all_anchors(image_shape=(640, 640), feature_strides=[16]):\n    \"\"\"Generate semua anchor untuk semua level feature map\"\"\"\n    all_anchors = []\n    base_anchors = generate_anchors(\n        base_size=16,\n        ratios=[0.5, 1.0, 2.0],\n        scales=[1, 1.26, 1.58]\n    )\n    \n    for stride in feature_strides:\n        # Generate grid\n        grid_width = image_shape[1] // stride\n        grid_height = image_shape[0] // stride\n        shift_x = np.arange(grid_width) * stride + stride//2  # Pusat anchor\n        shift_y = np.arange(grid_height) * stride + stride//2\n        \n        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n        shifts = np.stack([shift_x.ravel(), shift_y.ravel()], axis=1)\n        \n        # Generate level anchors\n        level_anchors = base_anchors.reshape((1, -1, 4)) + np.concatenate(\n            [shifts, shifts], axis=1).reshape((-1, 1, 4))\n        \n        all_anchors.append(level_anchors.reshape((-1, 4)))\n    \n    return np.concatenate(all_anchors, axis=0)\n\n# Generate anchors\nanchors = generate_all_anchors()\n\n# Filter anchors yang valid (berada dalam image boundary)\nvalid_anchors = anchors[\n    (anchors[:, 0] >= 0) & \n    (anchors[:, 1] >= 0) & \n    (anchors[:, 2] <= 640) & \n    (anchors[:, 3] <= 640)\n]\n\n# Visualisasi 100 anchor acak yang valid\nplt.figure(figsize=(12, 12))\nax = plt.gca()\nax.set_xlim([0, 640])\nax.set_ylim([640, 0])  # Image origin di kiri atas\n\nif len(valid_anchors) > 0:\n    sample_indices = np.random.choice(len(valid_anchors), min(100, len(valid_anchors)), replace=False)\n    for idx in sample_indices:\n        x1, y1, x2, y2 = valid_anchors[idx]\n        rect = Rectangle(\n            (x1, y1), \n            x2-x1, \n            y2-y1,\n            linewidth=1, \n            edgecolor='r', \n            facecolor='none',\n            alpha=0.5\n        )\n        ax.add_patch(rect)\nelse:\n    print(\"Tidak ada anchor yang valid!\")\n\nplt.title('Visualisasi Anchor Valid (640x640)\\n100 Anchor Acak', pad=20)\nplt.xlabel('Width (pixels)')\nplt.ylabel('Height (pixels)')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n# Print info anchor\nprint(f\"Total anchors generated: {len(anchors)}\")\nprint(f\"Valid anchors: {len(valid_anchors)}\")\nprint(f\"Sample anchor: {valid_anchors[0] if len(valid_anchors) > 0 else 'N/A'}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nfrom typing import Dict, Tuple, List\nfrom datasets import load_dataset\n\nclass RetinaNetDataGenerator(tf.keras.utils.Sequence):\n    def __init__(self, dataset, anchors, category_map, image_size=(640, 640), batch_size=4, shuffle=True, augment=False):\n        super().__init__()  # Fix PyDataset warning\n        self.dataset = dataset\n        self.anchors = anchors\n        self.category_map = category_map\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.augment = augment\n        self.indices = np.arange(len(dataset)).astype(int)\n        self.current_epoch = 0  # Track epoch for reshuffling\n        if shuffle:\n            np.random.shuffle(self.indices)\n\n    def __len__(self):\n        # Return 1000 untuk memaksa 1000 batch/epoch\n        return 1000\n\n    def __getitem__(self, idx):\n        # Gunakan modulo untuk cycle data\n        actual_idx = idx % len(self.dataset)\n        batch_indices = self.indices[actual_idx*self.batch_size : (actual_idx+1)*self.batch_size]\n        \n        batch_images, batch_cls, batch_reg = [], [], []\n        for i in batch_indices:\n            try:\n                example = self.dataset[int(i)]\n                \n                # Process image\n                image = np.array(example['image'])\n                image = tf.image.resize(image, self.image_size)\n                image = tf.cast(image, tf.float32) / 255.0\n\n                # === AUGMENTASI AMAN ===\n                if self.augment:\n                    if tf.random.uniform([]) < 0.5:\n                        image = tf.image.random_brightness(image, max_delta=0.2)\n                    if tf.random.uniform([]) < 0.5:\n                        image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n                    if tf.random.uniform([]) < 0.5:\n                        image = tf.image.random_saturation(image, lower=0.9, upper=1.1)\n                    if tf.random.uniform([]) < 0.2:\n                        image = tf.image.adjust_hue(image, delta=tf.random.uniform([], -0.05, 0.05))\n                \n                # Get ground truth\n                boxes, labels = self._get_gt_boxes_and_labels(json.loads(example[\"ground_truth\"]))\n                \n                # Match anchors\n                cls_target, reg_target = self._match_anchors_to_gt(boxes, labels)\n                \n                batch_images.append(image)\n                batch_cls.append(cls_target)\n                batch_reg.append(reg_target)\n                \n            except Exception as e:\n                print(f\"Error processing example {i}: {str(e)}\")\n                # Handle error dengan data dummy\n                dummy_image = np.zeros(self.image_size + (3,), dtype=np.float32)\n                dummy_cls = np.zeros(len(self.anchors), dtype=np.int32)\n                dummy_reg = np.zeros((len(self.anchors), 4), dtype=np.float32)\n                \n                batch_images.append(dummy_image)\n                batch_cls.append(dummy_cls)\n                batch_reg.append(dummy_reg)\n\n        # Stack dan return\n        return (\n            tf.stack(batch_images),\n            {\n                'class_output': tf.one_hot(tf.maximum(np.stack(batch_cls), 0), len(self.category_map)),\n                'bbox_output': tf.cast(np.stack(batch_reg), tf.float32)\n            }\n        )\n\n    def on_epoch_end(self):\n        self.current_epoch += 1\n        if self.shuffle:\n            np.random.shuffle(self.indices)  # Reshuffle setiap epoch\n        \n    def _get_gt_boxes_and_labels(self, example):\n        \"\"\"Extract boxes with proper type handling\"\"\"\n        boxes = []\n        labels = []\n        \n        for line in example.get('valid_line', []):\n            category = line.get('category', '')\n            if category not in self.category_map:\n                continue\n                \n            for word in line.get('words', []):\n                if 'quad' not in word:\n                    continue\n                    \n                # Convert coordinates to float32\n                x_coords = [float(word['quad'][f'x{i}']) for i in range(1, 5)]\n                y_coords = [float(word['quad'][f'y{i}']) for i in range(1, 5)]\n                \n                boxes.append([\n                    min(x_coords),\n                    min(y_coords),\n                    max(x_coords),\n                    max(y_coords)\n                ])\n                labels.append(int(self.category_map[category]))\n        \n        if not boxes:\n            return np.zeros((0, 4), dtype=np.float32), np.zeros((0,), dtype=np.int32)\n            \n        # Normalize to [0,1]\n        boxes = np.array(boxes, dtype=np.float32)\n        boxes[:, [0, 2]] /= self.image_size[1]  # width\n        boxes[:, [1, 3]] /= self.image_size[0]  # height\n        \n        return boxes, np.array(labels, dtype=np.int32)\n\n    def _match_anchors_to_gt(self, gt_boxes, gt_labels):\n        \"\"\"Match anchors to ground truth boxes\"\"\"\n        num_anchors = len(self.anchors)\n        cls_target = np.zeros(num_anchors, dtype=np.int32) - 1  # -1 means ignore\n        reg_target = np.zeros((num_anchors, 4), dtype=np.float32)\n        \n        if len(gt_boxes) == 0:\n            return cls_target, reg_target\n            \n        # Calculate IoU between anchors and GT boxes\n        iou_matrix = self._calculate_iou(gt_boxes)\n        \n        # Assign best match per GT\n        best_match = np.argmax(iou_matrix, axis=0)\n        best_iou = np.max(iou_matrix, axis=0)\n        \n        for gt_idx in np.where(best_iou > 0.5)[0]:\n            anchor_idx = best_match[gt_idx]\n            cls_target[anchor_idx] = gt_labels[gt_idx]\n            \n            # Calculate regression targets\n            gt_box = gt_boxes[gt_idx]\n            anchor = self.anchors[anchor_idx]\n            \n            reg_target[anchor_idx, 0] = (gt_box[0] - anchor[0]) / (anchor[2] - anchor[0])  # tx\n            reg_target[anchor_idx, 1] = (gt_box[1] - anchor[1]) / (anchor[3] - anchor[1])  # ty\n            reg_target[anchor_idx, 2] = np.log((gt_box[2] - gt_box[0]) / (anchor[2] - anchor[0]))  # tw\n            reg_target[anchor_idx, 3] = np.log((gt_box[3] - gt_box[1]) / (anchor[3] - anchor[1]))  # th\n        \n        return cls_target, reg_target\n\n    def _calculate_iou(self, gt_boxes):\n        \"\"\"Calculate IoU between anchors and GT boxes\"\"\"\n        # Convert to [x1,y1,x2,y2] format\n        anchors = self.anchors\n        gt_boxes = gt_boxes\n        \n        # Calculate intersection areas\n        inter_x1 = np.maximum(anchors[:, 0][:, None], gt_boxes[:, 0])\n        inter_y1 = np.maximum(anchors[:, 1][:, None], gt_boxes[:, 1])\n        inter_x2 = np.minimum(anchors[:, 2][:, None], gt_boxes[:, 2])\n        inter_y2 = np.minimum(anchors[:, 3][:, None], gt_boxes[:, 3])\n        \n        inter_area = np.maximum(inter_x2 - inter_x1, 0) * np.maximum(inter_y2 - inter_y1, 0)\n        \n        # Calculate union areas\n        anchor_area = (anchors[:, 2] - anchors[:, 0]) * (anchors[:, 3] - anchors[:, 1])\n        gt_area = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])\n        \n        union_area = anchor_area[:, None] + gt_area - inter_area\n        \n        return inter_area / (union_area + 1e-8)  # shape: (num_anchors, num_gt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"best_retinanet.keras\",\n    monitor=\"val_loss\",             # ganti ke 'val_loss' kalau pakai validation\n    save_best_only=True,\n    save_weights_only=False,\n    verbose=1\n)\n\nearlystop_cb = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    patience=5,\n    restore_best_weights=True\n)\n\ntensorboard_cb = tf.keras.callbacks.TensorBoard(\n    log_dir=\"./logs\",\n    update_freq=\"epoch\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\n\nanchors = generate_all_anchors()\n\noptimizer = Adam(\n    learning_rate=INITIAL_LR,\n    beta_1=MOMENTUM,\n    clipnorm=GRADIENT_CLIP_NORM\n)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss={\n        \"class_output\": tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n        \"bbox_output\": \"mse\",\n    },\n    metrics = {\n        \"class_output\": \"accuracy\",\n        \"bbox_output\":  \"mae\"\n    }\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.callbacks import CSVLogger\n\ncsv_logger = CSVLogger('/kaggle/working/training_log.csv', separator=',', append=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_gen = RetinaNetDataGenerator(\n        dataset=train_dataset,\n        batch_size=8,\n        image_size=(640, 640),\n        category_map=category_map\n    )\n    \nval_gen = RetinaNetDataGenerator(\n        dataset=val_dataset,\n        batch_size=8,\n        image_size=(640, 640),\n        category_map=category_map,\n        shuffle=False\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_gen,\n    validation_data=val_gen,\n    validation_steps=500,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    epochs=5,\n    callbacks=[checkpoint_cb, earlystop_cb, tensorboard_cb]\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\n\nlog_data = pd.read_csv('/kaggle/working/training_log.csv')\n\nplt.figure(figsize=(16, 12))\n\n# Plot untuk accuracy class_output\nplt.subplot(2, 2, 1)\nplt.title('Class Output Accuracy')\nplt.plot(log_data['epoch'], log_data['class_output_accuracy'], label='Training Accuracy')\nplt.plot(log_data['epoch'], log_data['val_class_output_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Plot untuk loss class_output\nplt.subplot(2, 2, 2)\nplt.title('Class Output Loss (Categorical Crossentropy)')\nplt.plot(log_data['epoch'], log_data['class_output_loss'], label='Training Loss')\nplt.plot(log_data['epoch'], log_data['val_class_output_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot untuk MAE bbox_output\nplt.subplot(2, 2, 3)\nplt.title('BBox Output MAE')\nplt.plot(log_data['epoch'], log_data['bbox_output_mae'], label='Training MAE')\nplt.plot(log_data['epoch'], log_data['val_bbox_output_mae'], label='Validation MAE')\nplt.xlabel('Epoch')\nplt.ylabel('MAE')\nplt.legend()\n\n# Plot untuk MSE loss bbox_output\nplt.subplot(2, 2, 4)\nplt.title('BBox Output Loss (MSE)')\nplt.plot(log_data['epoch'], log_data['bbox_output_loss'], label='Training Loss')\nplt.plot(log_data['epoch'], log_data['val_bbox_output_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('MSE Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"obj_resnet50v2_v2.h5\")\nmodel.save(\"obj_resnet50v2_v2.keras\")\nprint(\"Model saved successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.colab import auth\nauth.authenticate_user()\n\nfrom googleapiclient.discovery import build\nfrom googleapiclient.http import MediaFileUpload\nimport os","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drive_service = build('drive', 'v3')\n\nfolder_id = \"1a0i_DpxSo1RnkT7ZjUr_AdawVvW2vBSX\"  # Dapatkan dari URL Google Drive folder kamu\n\n# # # Zip folder SavedModel\n# # shutil.make_archive(\"final_faster_rcnn_resnet50v2\", 'zip', \"final_faster_rcnn_resnet50v2\")\n\n# # file_metadata = {\n# #     \"name\": \"final_faster_rcnn_resnet50v2\",\n# #     \"parents\": [folder_id]\n# # }\n# # media_savedmodel = MediaFileUpload(\"final_faster_rcnn_resnet50v2\", mimetype=\"application/octet-stream\")\n# # file_savedmodel = drive_service.files().create(body=file_metadata_savedmodel, media_body=media_savedmodel, fields=\"id\").execute()\n# # print(f\"File uploaded with ID: {file_savedmodel.get('id')}\")\n\nfile_metadata = {\n    \"name\": \"obj_resnet50v2_v2.keras\",\n    \"parents\": [folder_id]\n}\nmedia = MediaFileUpload(\"obj_resnet50v2_v2.keras\", mimetype=\"application/octet-stream\")\n\nfile = drive_service.files().create(body=file_metadata, media_body=media, fields=\"id\").execute()\nprint(f\"File uploaded with ID: {file.get('id')}\")\n\nfile_metadata_h5 = {\n    \"name\": \"obj_resnet50v2_v2.h5\",\n    \"parents\": [folder_id]\n}\nmedia_h5 = MediaFileUpload(\"obj_resnet50v2_v2.h5\", mimetype=\"application/octet-stream\")\n\nfile_h5 = drive_service.files().create(body=file_metadata_h5, media_body=media_h5, fields=\"id\").execute()\nprint(f\"File uploaded with ID: {file_h5.get('id')}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}