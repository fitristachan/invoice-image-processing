{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11948634,"sourceType":"datasetVersion","datasetId":7511867},{"sourceId":420857,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":343001,"modelId":364286}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GRAD CAM","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision matplotlib opencv-python\n!pip install ultralytics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:50:05.391274Z","iopub.execute_input":"2025-06-01T14:50:05.391618Z","iopub.status.idle":"2025-06-01T14:50:12.783288Z","shell.execute_reply.started":"2025-06-01T14:50:05.391597Z","shell.execute_reply":"2025-06-01T14:50:12.782075Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.146)\nRequirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.7.2)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (7.0.0)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.3)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.0->ultralytics) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.0->ultralytics) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.0->ultralytics) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom ultralytics import YOLO\nfrom torchvision import transforms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:50:12.784975Z","iopub.execute_input":"2025-06-01T14:50:12.785290Z","iopub.status.idle":"2025-06-01T14:50:12.790699Z","shell.execute_reply.started":"2025-06-01T14:50:12.785251Z","shell.execute_reply":"2025-06-01T14:50:12.789773Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model = YOLO(\"/kaggle/input/new_model/other/default/1/best.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:50:12.791806Z","iopub.execute_input":"2025-06-01T14:50:12.792078Z","iopub.status.idle":"2025-06-01T14:50:12.862903Z","shell.execute_reply.started":"2025-06-01T14:50:12.792051Z","shell.execute_reply":"2025-06-01T14:50:12.862010Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(\"\\n--- Layers in `model.model` (potential target_layer_name candidates) ---\")\ntry:\n    # Loop melalui generator yang dikembalikan oleh model.model.named_modules()\n    for name, module in model.model.named_modules():\n        # Anda bisa menambahkan filter di sini jika daftarnya terlalu panjang\n        # Misalnya, hanya tampilkan layer yang relevan untuk Grad-CAM\n        if isinstance(module, torch.nn.Conv2d) or \\\n           'Conv' in str(type(module).__name__) or \\\n           'C3'   in str(type(module).__name__) or \\\n           'SPPF' in str(type(module).__name__) or \\\n           'Bottleneck' in str(type(module).__name__):\n            print(f\"Name: '{name}', Module: {type(module).__name__}\")\nexcept AttributeError:\n    print(\"Error: 'model.model' tidak ditemukan atau bukan nn.Module. Pastikan model sudah di-load dengan benar.\")\n    print(\"Jika model Anda adalah objek YOLO dari Ultralytics, 'model.model' seharusnya adalah modul PyTorch internal.\")\nexcept Exception as e:\n    print(f\"Terjadi error saat mengakses layer: {e}\")\n\nprint(\"---------------------------------------------------------------------\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:50:12.864571Z","iopub.execute_input":"2025-06-01T14:50:12.864829Z","iopub.status.idle":"2025-06-01T14:50:12.872758Z","shell.execute_reply.started":"2025-06-01T14:50:12.864811Z","shell.execute_reply":"2025-06-01T14:50:12.871763Z"}},"outputs":[{"name":"stdout","text":"\n--- Layers in `model.model` (potential target_layer_name candidates) ---\nName: 'model.0', Module: Conv\nName: 'model.0.conv', Module: Conv2d\nName: 'model.1', Module: Conv\nName: 'model.1.conv', Module: Conv2d\nName: 'model.2.cv1', Module: Conv\nName: 'model.2.cv1.conv', Module: Conv2d\nName: 'model.2.cv2', Module: Conv\nName: 'model.2.cv2.conv', Module: Conv2d\nName: 'model.2.m.0', Module: Bottleneck\nName: 'model.2.m.0.cv1', Module: Conv\nName: 'model.2.m.0.cv1.conv', Module: Conv2d\nName: 'model.2.m.0.cv2', Module: Conv\nName: 'model.2.m.0.cv2.conv', Module: Conv2d\nName: 'model.3', Module: Conv\nName: 'model.3.conv', Module: Conv2d\nName: 'model.4.cv1', Module: Conv\nName: 'model.4.cv1.conv', Module: Conv2d\nName: 'model.4.cv2', Module: Conv\nName: 'model.4.cv2.conv', Module: Conv2d\nName: 'model.4.m.0', Module: Bottleneck\nName: 'model.4.m.0.cv1', Module: Conv\nName: 'model.4.m.0.cv1.conv', Module: Conv2d\nName: 'model.4.m.0.cv2', Module: Conv\nName: 'model.4.m.0.cv2.conv', Module: Conv2d\nName: 'model.4.m.1', Module: Bottleneck\nName: 'model.4.m.1.cv1', Module: Conv\nName: 'model.4.m.1.cv1.conv', Module: Conv2d\nName: 'model.4.m.1.cv2', Module: Conv\nName: 'model.4.m.1.cv2.conv', Module: Conv2d\nName: 'model.5', Module: Conv\nName: 'model.5.conv', Module: Conv2d\nName: 'model.6.cv1', Module: Conv\nName: 'model.6.cv1.conv', Module: Conv2d\nName: 'model.6.cv2', Module: Conv\nName: 'model.6.cv2.conv', Module: Conv2d\nName: 'model.6.m.0', Module: Bottleneck\nName: 'model.6.m.0.cv1', Module: Conv\nName: 'model.6.m.0.cv1.conv', Module: Conv2d\nName: 'model.6.m.0.cv2', Module: Conv\nName: 'model.6.m.0.cv2.conv', Module: Conv2d\nName: 'model.6.m.1', Module: Bottleneck\nName: 'model.6.m.1.cv1', Module: Conv\nName: 'model.6.m.1.cv1.conv', Module: Conv2d\nName: 'model.6.m.1.cv2', Module: Conv\nName: 'model.6.m.1.cv2.conv', Module: Conv2d\nName: 'model.7', Module: Conv\nName: 'model.7.conv', Module: Conv2d\nName: 'model.8.cv1', Module: Conv\nName: 'model.8.cv1.conv', Module: Conv2d\nName: 'model.8.cv2', Module: Conv\nName: 'model.8.cv2.conv', Module: Conv2d\nName: 'model.8.m.0', Module: Bottleneck\nName: 'model.8.m.0.cv1', Module: Conv\nName: 'model.8.m.0.cv1.conv', Module: Conv2d\nName: 'model.8.m.0.cv2', Module: Conv\nName: 'model.8.m.0.cv2.conv', Module: Conv2d\nName: 'model.9', Module: SPPF\nName: 'model.9.cv1', Module: Conv\nName: 'model.9.cv1.conv', Module: Conv2d\nName: 'model.9.cv2', Module: Conv\nName: 'model.9.cv2.conv', Module: Conv2d\nName: 'model.12.cv1', Module: Conv\nName: 'model.12.cv1.conv', Module: Conv2d\nName: 'model.12.cv2', Module: Conv\nName: 'model.12.cv2.conv', Module: Conv2d\nName: 'model.12.m.0', Module: Bottleneck\nName: 'model.12.m.0.cv1', Module: Conv\nName: 'model.12.m.0.cv1.conv', Module: Conv2d\nName: 'model.12.m.0.cv2', Module: Conv\nName: 'model.12.m.0.cv2.conv', Module: Conv2d\nName: 'model.15.cv1', Module: Conv\nName: 'model.15.cv1.conv', Module: Conv2d\nName: 'model.15.cv2', Module: Conv\nName: 'model.15.cv2.conv', Module: Conv2d\nName: 'model.15.m.0', Module: Bottleneck\nName: 'model.15.m.0.cv1', Module: Conv\nName: 'model.15.m.0.cv1.conv', Module: Conv2d\nName: 'model.15.m.0.cv2', Module: Conv\nName: 'model.15.m.0.cv2.conv', Module: Conv2d\nName: 'model.16', Module: Conv\nName: 'model.16.conv', Module: Conv2d\nName: 'model.18.cv1', Module: Conv\nName: 'model.18.cv1.conv', Module: Conv2d\nName: 'model.18.cv2', Module: Conv\nName: 'model.18.cv2.conv', Module: Conv2d\nName: 'model.18.m.0', Module: Bottleneck\nName: 'model.18.m.0.cv1', Module: Conv\nName: 'model.18.m.0.cv1.conv', Module: Conv2d\nName: 'model.18.m.0.cv2', Module: Conv\nName: 'model.18.m.0.cv2.conv', Module: Conv2d\nName: 'model.19', Module: Conv\nName: 'model.19.conv', Module: Conv2d\nName: 'model.21.cv1', Module: Conv\nName: 'model.21.cv1.conv', Module: Conv2d\nName: 'model.21.cv2', Module: Conv\nName: 'model.21.cv2.conv', Module: Conv2d\nName: 'model.21.m.0', Module: Bottleneck\nName: 'model.21.m.0.cv1', Module: Conv\nName: 'model.21.m.0.cv1.conv', Module: Conv2d\nName: 'model.21.m.0.cv2', Module: Conv\nName: 'model.21.m.0.cv2.conv', Module: Conv2d\nName: 'model.22.cv2.0.0', Module: Conv\nName: 'model.22.cv2.0.0.conv', Module: Conv2d\nName: 'model.22.cv2.0.1', Module: Conv\nName: 'model.22.cv2.0.1.conv', Module: Conv2d\nName: 'model.22.cv2.0.2', Module: Conv2d\nName: 'model.22.cv2.1.0', Module: Conv\nName: 'model.22.cv2.1.0.conv', Module: Conv2d\nName: 'model.22.cv2.1.1', Module: Conv\nName: 'model.22.cv2.1.1.conv', Module: Conv2d\nName: 'model.22.cv2.1.2', Module: Conv2d\nName: 'model.22.cv2.2.0', Module: Conv\nName: 'model.22.cv2.2.0.conv', Module: Conv2d\nName: 'model.22.cv2.2.1', Module: Conv\nName: 'model.22.cv2.2.1.conv', Module: Conv2d\nName: 'model.22.cv2.2.2', Module: Conv2d\nName: 'model.22.cv3.0.0', Module: Conv\nName: 'model.22.cv3.0.0.conv', Module: Conv2d\nName: 'model.22.cv3.0.1', Module: Conv\nName: 'model.22.cv3.0.1.conv', Module: Conv2d\nName: 'model.22.cv3.0.2', Module: Conv2d\nName: 'model.22.cv3.1.0', Module: Conv\nName: 'model.22.cv3.1.0.conv', Module: Conv2d\nName: 'model.22.cv3.1.1', Module: Conv\nName: 'model.22.cv3.1.1.conv', Module: Conv2d\nName: 'model.22.cv3.1.2', Module: Conv2d\nName: 'model.22.cv3.2.0', Module: Conv\nName: 'model.22.cv3.2.0.conv', Module: Conv2d\nName: 'model.22.cv3.2.1', Module: Conv\nName: 'model.22.cv3.2.1.conv', Module: Conv2d\nName: 'model.22.cv3.2.2', Module: Conv2d\nName: 'model.22.dfl.conv', Module: Conv2d\n---------------------------------------------------------------------\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nimport gc\n\n# --- Kelas GradCAM ---\nclass GradCAM:\n    def __init__(self, model, target_layer_name):\n        self.model_wrapper = model # Objek YOLO dari Ultralytics\n        self.actual_model_module = model.model # nn.Module internal (misal, DetectionModel)\n        self.target_layer_name = target_layer_name\n        self.feature_maps = None\n        self.gradients = None\n\n        self.actual_model_module.eval()\n        self._register_hooks()\n\n    def _register_hooks(self):\n        found_layer = False\n        # Mencari di dalam self.actual_model_module (model.model)\n        for name, module in self.actual_model_module.named_modules():\n            if name == self.target_layer_name:\n                # (Definisi forward_hook dan backward_hook sama seperti sebelumnya)\n                def forward_hook(module, input, output):\n                    self.feature_maps = output.detach()\n                def backward_hook(module, grad_input, grad_output):\n                    self.gradients = grad_output[0].detach()\n\n                module.register_forward_hook(forward_hook)\n                module.register_full_backward_hook(backward_hook)\n                found_layer = True\n                print(f\"Hooks registered on layer: {self.target_layer_name} within {type(self.actual_model_module).__name__}\")\n                break\n        \n        if not found_layer:\n            # Jika tidak ketemu di model.model, coba di model_wrapper (objek YOLO utama)\n            # Ini untuk kasus jika target_layer_name menyertakan prefix 'model.'\n            print(f\"Layer '{self.target_layer_name}' not found directly in '{type(self.actual_model_module).__name__}'. Trying in YOLO object root...\")\n            for name, module in self.model_wrapper.named_modules():\n                 if name == self.target_layer_name:\n                    def forward_hook(module, input, output): # Perlu didefinisikan ulang di scope ini\n                        self.feature_maps = output.detach()\n                    def backward_hook(module, grad_input, grad_output): # Perlu didefinisikan ulang\n                        self.gradients = grad_output[0].detach()\n                    \n                    module.register_forward_hook(forward_hook)\n                    module.register_full_backward_hook(backward_hook)\n                    found_layer = True\n                    print(f\"Hooks registered on layer: {self.target_layer_name} within {type(self.model_wrapper).__name__} (root)\")\n                    break\n        \n        if not found_layer:\n            raise ValueError(\n                f\"Target layer '{self.target_layer_name}' not found. \\n\"\n                f\"Please inspect `model.model.named_modules()` and `model.named_modules()`.\"\n            )\n\n    def _get_target_score_for_detection(self, list_of_head_outputs, target_class_id, device):\n        total_score = torch.tensor(0.0, device=device, requires_grad=True)\n        \n        if not list_of_head_outputs: # Jika list_of_head_outputs kosong\n            print(\"DEBUG: list_of_head_outputs is empty. Returning zero score.\")\n            return total_score\n\n        # Dapatkan nc (jumlah kelas) dari model\n        # self.actual_model_module adalah DetectionModel, yang biasanya punya atribut .nc\n        num_classes_from_model = self.actual_model_module.nc if hasattr(self.actual_model_module, 'nc') else 80\n        print(f\"DEBUG _get_target_score_for_detection: num_classes_from_model (nc) = {num_classes_from_model}\")\n        # Untuk YOLOv8, format outputnya: box(4) + cls_logits(num_classes)\n        expected_features_yolov8 = 4 + num_classes_from_model\n        \n        processed_any_tensor = False\n        for i, pred_tensor_original_shape in enumerate(list_of_head_outputs):\n            if not isinstance(pred_tensor_original_shape, torch.Tensor):\n                print(f\"DEBUG _get_target_score_for_detection: Element {i} in list_of_head_outputs is not a Tensor (type: {type(pred_tensor_original_shape)}). Skipping.\")\n                continue\n\n            print(f\"DEBUG _get_target_score_for_detection: Processing head output {i} with original shape {pred_tensor_original_shape.shape}\")\n            pred_tensor = pred_tensor_original_shape.clone() # Bekerja dengan salinan\n\n            if pred_tensor.shape[0] != 1:\n                print(f\"Warning _get_target_score_for_detection: Batch size of head output {i} is {pred_tensor.shape[0]} != 1. Taking first element.\")\n                pred_tensor = pred_tensor[0:1]\n\n            # Output dari head YOLO biasanya [batch, channels, height, width]\n            # Channels = num_anchors * (5 + num_classes) atau num_anchors * (4 + num_classes) untuk v8\n            # Kita perlu me-reshape menjadi [batch, num_total_predictions_this_head, 5/4 + num_classes]\n\n            bs, _, H, W = pred_tensor.shape # Asumsi [B, C_total, H, W]\n            # C_total = num_anchors * (box_params + num_classes)\n            # Untuk YOLOv8, num_anchors biasanya 1 (karena anchor-free)\n            # Dan outputnya box(4) + num_classes, jadi no = 4 + num_classes\n            # Maka C_total = 4 + num_classes (jika tidak ada reg_max untuk DFL)\n            # Atau C_total = 4 + num_classes + reg_max*num_box_coords (jika ada DFL)\n\n            # Untuk YOLOv8, Detect().forward() sudah melakukan reshape menjadi [bs, self.na * self.no, self.nl]\n            # dan dikembalikan sebagai list. Output dari Detect().forward(x) adalah:\n            # x adalah list [P3_out, P4_out, P5_out]\n            # P_out shape: [bs, num_channels (box+cls), H, W]\n            # Di Detect().forward() sebelum return:\n            # x[i] = x[i].view(bs, self.na, self.no, hei, wid).permute(0, 1, 3, 4, 2).contiguous()\n            #   shape [bs, self.na, H, W, self.no (4+nc)]\n            # Kemudian: return x if self.export else (torch.cat(list(p.view(bs, -1, self.no) for p in x), 1), x)\n            # Jadi, elemen-elemen di list_of_head_outputs (yaitu 'x' kedua di tuple return)\n            # memiliki shape [bs, self.na, H, W, self.no]\n            \n            if pred_tensor.dim() == 4: # Jika [B, C, H, W]\n                # Ini mungkin output sebelum Detect layer melakukan view/permute akhirnya.\n                # Atau, jika na=1, C = self.no\n                # Ini perlu penyesuaian berdasarkan pemahaman yang lebih dalam tentang `x[i]` sebelum `torch.cat`\n                # Untuk sementara, asumsikan ini adalah salah satu output feature map yang sudah di-decode sebagian oleh Detect layer\n                # Jika nc benar dan C_total = 4+nc, kita bisa reshape\n                pred_tensor = pred_tensor.view(bs, num_classes_from_model + 4, -1).permute(0, 2, 1).contiguous()\n                # Sekarang jadi [bs, H*W, 4+nc]\n                print(f\"DEBUG _get_target_score_for_detection: Reshaped head output {i} to {pred_tensor.shape} (from 4D)\")\n\n\n            elif pred_tensor.dim() == 5: # Jika [B, NumAnchors, H, W, Features(4+nc)]\n                pred_tensor = pred_tensor.reshape(pred_tensor.shape[0], -1, pred_tensor.shape[-1])\n                print(f\"DEBUG _get_target_score_for_detection: Reshaped head output {i} to {pred_tensor.shape} (from 5D)\")\n            \n            if pred_tensor.dim() != 3:\n                print(f\"DEBUG _get_target_score_for_detection: Head output {i} has dims {pred_tensor.shape} after reshape, expected 3D ([1, N, C]). Skipping.\")\n                continue\n            \n            num_features_last_dim = pred_tensor.shape[-1]\n\n            if num_features_last_dim == expected_features_yolov8:\n                class_logits = pred_tensor[..., 4:] # Box: 0-3, Class logits: 4-end\n                class_probs = torch.sigmoid(class_logits)\n                target_class_prob = class_probs[..., target_class_id]\n                current_score_component = target_class_prob.sum()\n                total_score = total_score + current_score_component\n                processed_any_tensor = True\n                print(f\"DEBUG _get_target_score_for_detection: Processed head {i} (YOLOv8 format). Score component: {current_score_component.item()}\")\n            else:\n                print(f\"DEBUG _get_target_score_for_detection: Last dim of head output {i} ({num_features_last_dim}) \"\n                      f\"does not match expected_features_yolov8 ({expected_features_yolov8}). Skipping.\")\n\n        if not processed_any_tensor:\n            print(\"DEBUG _get_target_score_for_detection: No head output tensors were successfully processed to calculate score.\")\n        \n        print(f\"DEBUG _get_target_score_for_detection: Final total_score = {total_score.item()}\")\n        return total_score\n\n    def __call__(self, input_tensor, target_class_id):\n        if input_tensor.grad is not None:\n            input_tensor.grad.zero_()\n        input_tensor.requires_grad_(True)\n\n        model_output_tuple_or_list = self.actual_model_module(input_tensor)\n        \n        # --- DEBUGGING OUTPUT MODEL ---\n        print(f\"DEBUG __call__: type(model_output_tuple_or_list) = {type(model_output_tuple_or_list)}\")\n        if isinstance(model_output_tuple_or_list, (list, tuple)):\n            print(f\"DEBUG __call__: len(model_output_tuple_or_list) = {len(model_output_tuple_or_list)}\")\n            for i_debug, item_debug in enumerate(model_output_tuple_or_list):\n                if isinstance(item_debug, torch.Tensor):\n                    print(f\"DEBUG __call__: model_output_tuple_or_list[{i_debug}] is Tensor, shape: {item_debug.shape}\")\n                elif isinstance(item_debug, list):\n                    print(f\"DEBUG __call__: model_output_tuple_or_list[{i_debug}] is List, len: {len(item_debug)}\")\n                    for j_debug, sub_item_debug in enumerate(item_debug):\n                         if isinstance(sub_item_debug, torch.Tensor):\n                              print(f\"DEBUG __call__:   model_output_tuple_or_list[{i_debug}][{j_debug}] is Tensor, shape: {sub_item_debug.shape}\")\n                         else:\n                              print(f\"DEBUG __call__:   model_output_tuple_or_list[{i_debug}][{j_debug}] is {type(sub_item_debug)}\")\n                else:\n                    print(f\"DEBUG __call__: model_output_tuple_or_list[{i_debug}] is {type(item_debug)}\")\n        # --- END DEBUGGING ---\n\n        list_of_head_outputs_for_score = []\n        if isinstance(model_output_tuple_or_list, tuple) and \\\n           len(model_output_tuple_or_list) > 0 and \\\n           isinstance(model_output_tuple_or_list[-1], list):\n            # Kasus umum untuk Detect().forward() dalam mode eval: return (cat_preds, list_of_head_outputs)\n            # Kita ambil list_of_head_outputs nya\n            list_of_head_outputs_for_score = model_output_tuple_or_list[-1]\n            print(\"DEBUG __call__: Extracted list_of_head_outputs (second element of tuple) for score calculation.\")\n        elif isinstance(model_output_tuple_or_list, list) and \\\n             all(isinstance(t, torch.Tensor) for t in model_output_tuple_or_list):\n            # Jika outputnya sudah berupa list tensor (misal, mode training atau model berbeda)\n            list_of_head_outputs_for_score = model_output_tuple_or_list\n            print(\"DEBUG __call__: Using the direct list output (all tensors) for score calculation.\")\n        else:\n            print(f\"ERROR __call__: Unexpected output structure from model.model: type is {type(model_output_tuple_or_list)}. \"\n                  \"Cannot reliably determine raw head outputs for Grad-CAM.\")\n            # Fallback, biarkan _get_target_score_for_detection menangani list kosong\n        \n        score = self._get_target_score_for_detection(list_of_head_outputs_for_score, target_class_id, input_tensor.device)\n\n        if self.feature_maps is None:\n             raise RuntimeError(\"Feature maps were not captured. Check `target_layer_name` and hooks registration path.\")\n        \n        del model_output_tuple_or_list # Hapus output model mentah\n        if torch.cuda.is_available(): torch.cuda.empty_cache()\n        gc.collect()\n\n        if score.item() == 0.0:\n            print(\"Warning: Grad-CAM target score is 0.0. Heatmap will be empty.\")\n            return np.zeros(self.feature_maps.shape[2:], dtype=np.float32)\n\n        score.backward()\n\n        if self.gradients is None:\n            raise RuntimeError(\"Gradients were not captured. Check `target_layer_name`, hooks, \"\n                               \"and if `score` is connected to the graph involving `target_layer_name`.\")\n\n        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n        weighted_feature_maps = self.feature_maps[0] * pooled_gradients[:, None, None]\n        heatmap = torch.sum(weighted_feature_maps, dim=0)\n        heatmap = F.relu(heatmap)\n\n        if heatmap.max() > 0: heatmap /= heatmap.max()\n        \n        del score, pooled_gradients, weighted_feature_maps \n        # Jangan hapus self.feature_maps dan self.gradients di sini jika GradCAM instance mau dipakai ulang,\n        # tapi karena kita buat instance baru tiap kali, tidak masalah.\n        # Atau set ke None agar jelas\n        current_fm = self.feature_maps # simpan referensi untuk dihapus\n        current_grad = self.gradients # simpan referensi untuk dihapus\n        self.feature_maps = None \n        self.gradients = None   \n        del current_fm, current_grad\n\n        if torch.cuda.is_available(): torch.cuda.empty_cache()\n        gc.collect()\n        \n        return heatmap.cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:50:12.873906Z","iopub.execute_input":"2025-06-01T14:50:12.874309Z","iopub.status.idle":"2025-06-01T14:50:12.902288Z","shell.execute_reply.started":"2025-06-01T14:50:12.874278Z","shell.execute_reply":"2025-06-01T14:50:12.901575Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"if __name__ == '__main__':\n    # Set device di awal\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Script will attempt to run on: {device}\")\n\n    # 1. Load model YOLO Anda\n    model_path = \"/kaggle/input/new_model/other/default/1/best.pt\"\n    try:\n        # Load model tanpa langsung memindahkannya ke device dengan .to(device) pada objek YOLO\n        model_wrapper = YOLO(model_path) # Ini adalah objek YOLO wrapper\n        \n        # Pindahkan modul internal PyTorch (model.model) ke device yang diinginkan\n        # Ini adalah langkah penting untuk memastikan semua weight ada di device yang benar\n        model_wrapper.model.to(device) \n        \n        print(f\"Model YOLO loaded from: {model_path}\")\n        # Verifikasi device dari salah satu parameter model.model\n        # Ini untuk memastikan model.model benar-benar sudah pindah\n        first_param_device = next(model_wrapper.model.parameters()).device\n        print(f\"Model's internal module (model.model) is on device: {first_param_device}\")\n\n\n        # VERIFIKASI TARGET LAYER NAME (Jalankan ini di sel terpisah jika perlu)\n        # print(\"\\n--- Layers in `model.model` (potential target_layer_name candidates) ---\")\n        # for name, module in model_wrapper.model.named_modules():\n        #     if isinstance(module, torch.nn.Conv2d) or \\\n        #        'Conv' in str(type(module).__name__) or \\\n        #        'C3'   in str(type(module).__name__) or \\\n        #        'SPPF' in str(type(module).__name__) or \\\n        #        'Bottleneck' in str(type(module).__name__):\n        #         print(name)\n        # print(\"---------------------------------------------------------------------\\n\")\n\n    except Exception as e:\n        print(f\"Error loading YOLO model: {e}\")\n        import traceback\n        traceback.print_exc()\n        exit()\n\n    #\"model.9.cv2.conv\"\n    # model.21.m.0.cv2.conv\n    target_layer_name = \"model.9.cv2.conv\" # Anda sudah benar di sini\n    print(f\"Target layer name set to: {target_layer_name}\")\n\n\n    # 2. Load dan preprocess gambar Anda\n    image_path = \"/kaggle/input/explainable/contoh-1.jpeg\"\n    try:\n        original_img_rgb = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        print(f\"Image loaded from: {image_path}\")\n    except Exception as e:\n        print(f\"Error loading image: {e}\")\n        import traceback\n        traceback.print_exc()\n        exit()\n\n    input_size = (640, 640) # Tetap gunakan ukuran kecil dulu untuk tes\n    print(f\"Menggunakan ukuran input: {input_size}\")\n\n    img_resized = cv2.resize(original_img_rgb, input_size)\n    img_normalized = img_resized / 255.0\n    img_transposed = np.transpose(img_normalized, (2, 0, 1))\n    img_tensor = torch.from_numpy(img_transposed).float().unsqueeze(0).to(device) # Pastikan tensor input juga di device yang sama\n\n    # 3. Tentukan target class ID\n    target_class_id_for_gradcam = 0\n    if hasattr(model_wrapper, 'names') and model_wrapper.names:\n        print(f\"Targeting class ID: {target_class_id_for_gradcam} ('{model_wrapper.names[target_class_id_for_gradcam]}')\")\n    else:\n        print(f\"Targeting class ID: {target_class_id_for_gradcam} (model.names not available or empty)\")\n\n\n    # 4. Buat instance GradCAM dan hasilkan heatmap\n    try:\n        print(\"Initializing GradCAM...\")\n        # Pass objek YOLO wrapper ke GradCAM\n        grad_cam_executor = GradCAM(model_wrapper, target_layer_name) \n        print(\"Generating Grad-CAM heatmap...\")\n        heatmap_gradcam = grad_cam_executor(img_tensor, target_class_id_for_gradcam)\n        print(\"Grad-CAM heatmap generated.\")\n\n        # ... (sisa kode visualisasi sama) ...\n        if heatmap_gradcam is not None and heatmap_gradcam.size > 0 :\n            heatmap_display_size = (original_img_rgb.shape[1], original_img_rgb.shape[0])\n            heatmap_resized = cv2.resize(heatmap_gradcam, heatmap_display_size)\n            heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), cv2.COLORMAP_JET)\n\n            overlayed_image_bgr = cv2.addWeighted(cv2.cvtColor(original_img_rgb, cv2.COLOR_RGB2BGR), 0.5, heatmap_colored, 0.5, 0)\n            \n            plt.figure(figsize=(18, 6))\n            plt.subplot(1, 3, 1)\n            plt.imshow(original_img_rgb)\n            plt.title(\"Original Image\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 2)\n            plt.imshow(heatmap_resized, cmap='jet')\n            plt.title(\"Grad-CAM Heatmap\")\n            plt.axis(\"off\")\n\n            plt.subplot(1, 3, 3)\n            plt.imshow(cv2.cvtColor(overlayed_image_bgr, cv2.COLOR_BGR2RGB))\n            plt.title(f\"Overlayed Grad-CAM (Class ID: {target_class_id_for_gradcam})\")\n            plt.axis(\"off\")\n            plt.suptitle(f\"Grad-CAM: Layer '{target_layer_name}' @ {input_size}\", fontsize=14)\n            plt.tight_layout(rect=[0, 0, 1, 0.96])\n            plt.show()\n        else:\n            print(\"Grad-CAM heatmap is empty or None, skipping visualization.\")\n\n\n    except Exception as e:\n        print(f\"An error occurred during Grad-CAM execution: {e}\")\n        import traceback\n        traceback.print_exc()\n    finally:\n        # Selalu coba bersihkan memori setelah selesai atau jika ada error\n        if 'model_wrapper' in locals():\n            del model_wrapper\n        if 'img_tensor' in locals():\n            del img_tensor \n        if 'grad_cam_executor' in locals():\n            del grad_cam_executor\n        if 'heatmap_gradcam' in locals() and heatmap_gradcam is not None:\n            del heatmap_gradcam\n        \n        import gc # Pindahkan import gc ke atas jika belum\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        print(\"Memory cleanup attempted.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T14:50:12.903327Z","iopub.execute_input":"2025-06-01T14:50:12.903620Z"}},"outputs":[{"name":"stdout","text":"Script will attempt to run on: cpu\nModel YOLO loaded from: /kaggle/input/new_model/other/default/1/best.pt\nModel's internal module (model.model) is on device: cpu\nTarget layer name set to: model.9.cv2.conv\nImage loaded from: /kaggle/input/explainable/contoh-1.jpeg\nMenggunakan ukuran input: (640, 640)\nTargeting class ID: 0 ('table')\nInitializing GradCAM...\nHooks registered on layer: model.9.cv2.conv within DetectionModel\nGenerating Grad-CAM heatmap...\nDEBUG __call__: type(model_output_tuple_or_list) = <class 'tuple'>\nDEBUG __call__: len(model_output_tuple_or_list) = 2\nDEBUG __call__: model_output_tuple_or_list[0] is Tensor, shape: torch.Size([1, 6, 8400])\nDEBUG __call__: model_output_tuple_or_list[1] is List, len: 3\nDEBUG __call__:   model_output_tuple_or_list[1][0] is Tensor, shape: torch.Size([1, 66, 80, 80])\nDEBUG __call__:   model_output_tuple_or_list[1][1] is Tensor, shape: torch.Size([1, 66, 40, 40])\nDEBUG __call__:   model_output_tuple_or_list[1][2] is Tensor, shape: torch.Size([1, 66, 20, 20])\nDEBUG __call__: Extracted list_of_head_outputs (second element of tuple) for score calculation.\nDEBUG _get_target_score_for_detection: num_classes_from_model (nc) = 2\nDEBUG _get_target_score_for_detection: Processing head output 0 with original shape torch.Size([1, 66, 80, 80])\nDEBUG _get_target_score_for_detection: Reshaped head output 0 to torch.Size([1, 70400, 6]) (from 4D)\nDEBUG _get_target_score_for_detection: Processed head 0 (YOLOv8 format). Score component: 43288.7421875\nDEBUG _get_target_score_for_detection: Processing head output 1 with original shape torch.Size([1, 66, 40, 40])\nDEBUG _get_target_score_for_detection: Reshaped head output 1 to torch.Size([1, 17600, 6]) (from 4D)\nDEBUG _get_target_score_for_detection: Processed head 1 (YOLOv8 format). Score component: 11580.013671875\nDEBUG _get_target_score_for_detection: Processing head output 2 with original shape torch.Size([1, 66, 20, 20])\nDEBUG _get_target_score_for_detection: Reshaped head output 2 to torch.Size([1, 4400, 6]) (from 4D)\nDEBUG _get_target_score_for_detection: Processed head 2 (YOLOv8 format). Score component: 2721.91796875\nDEBUG _get_target_score_for_detection: Final total_score = 57590.67578125\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Sailency Map","metadata":{}},{"cell_type":"code","source":"import torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom torchvision import transforms\nfrom ultralytics import YOLO","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    model = YOLO('/kaggle/input/new_model/other/default/1/best.pt')\nexcept Exception as e:\n    print(f\"Error loading YOLO model: {e}\")\n    exit()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_image(image_path):\n    image = Image.open(image_path).convert('RGB')\n    transform = transforms.Compose([\n        transforms.Resize((640, 640)), \n        transforms.ToTensor(),\n    ])\n    img_tensor = transform(image)\n    return image, img_tensor.unsqueeze(0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport gc\nimport traceback # Untuk mencetak traceback lengkap jika ada Python exception\n\n# ... (fungsi preprocess_image, visualize_saliency, dan load model Anda) ...\n\ndef generate_saliency_map(pytorch_module, input_image_tensor, target_class_idx_for_saliency=0):\n    saliency_np = None\n    try:\n        print(\"[SALIENCY DEBUG] Memulai generate_saliency_map...\")\n        device = next(pytorch_module.parameters()).device\n        print(f\"[SALIENCY DEBUG] Model (pytorch_module) ada di perangkat: {device}\")\n\n        input_image_tensor = input_image_tensor.to(device)\n        if not input_image_tensor.requires_grad:\n            input_image_tensor.requires_grad_(True)\n        print(f\"[SALIENCY DEBUG] input_image_tensor dipindahkan ke {device} dan requires_grad={input_image_tensor.requires_grad}\")\n\n        original_training_mode = pytorch_module.training\n        print(f\"[SALIENCY DEBUG] Mode training asli modul: {original_training_mode}\")\n        raw_predictions = None\n        try:\n            pytorch_module.train()\n            print(\"[SALIENCY DEBUG] Mode modul diubah ke train(). Melakukan forward pass...\")\n            raw_predictions = pytorch_module(input_image_tensor)\n            print(\"[SALIENCY DEBUG] Forward pass selesai.\")\n        finally:\n            pytorch_module.train(original_training_mode)\n            print(f\"[SALIENCY DEBUG] Mode training modul dikembalikan ke: {original_training_mode}\")\n\n        if raw_predictions is None:\n            print(\"[SALIENCY ERROR] raw_predictions adalah None setelah forward pass.\")\n            return None\n        \n        print(\"[SALIENCY DEBUG] Memilih target_score...\")\n        target_score = None\n        # Menggunakan nama variabel yang berbeda untuk menghindari kebingungan dengan tensor yang dihapus\n        main_pred_tensor_from_raw = None \n        \n        if isinstance(raw_predictions, (tuple, list)) and len(raw_predictions) > 0 and isinstance(raw_predictions[0], torch.Tensor):\n            main_pred_tensor_from_raw = raw_predictions[0]\n            if len(raw_predictions) > 1: del raw_predictions[1:] # Hapus sisa jika ada\n        elif isinstance(raw_predictions, torch.Tensor):\n            main_pred_tensor_from_raw = raw_predictions\n        \n        del raw_predictions # Hapus variabel raw_predictions asli\n        gc.collect()\n        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n        print(\"[SALIENCY DEBUG] Referensi raw_predictions (variabel) dihapus dan cache dikosongkan.\")\n\n        if main_pred_tensor_from_raw is not None:\n            print(f\"[SALIENCY DEBUG] Shape main_pred_tensor_from_raw: {main_pred_tensor_from_raw.shape}\") # Harusnya [1, 66, 80, 80]\n\n            # --- KOREKSI LOGIKA TARGET SCORE untuk shape [B, C, H, W] ---\n            # C = 66. Asumsi: 4 (bbox) + 1 (objectness) + 61 (classes)\n            obj_score_channel_idx = 4\n            class_scores_start_channel_idx = 5\n\n            if target_class_idx_for_saliency is not None:\n                actual_channel_for_class = class_scores_start_channel_idx + target_class_idx_for_saliency\n                # Pastikan channel index valid untuk dimensi ke-1 (channels)\n                if main_pred_tensor_from_raw.shape[1] > actual_channel_for_class:\n                    target_score = main_pred_tensor_from_raw[0, actual_channel_for_class, :, :].sum()\n                    print(f\"[SALIENCY DEBUG] Menggunakan jumlah skor (sum over H,W) untuk kelas {target_class_idx_for_saliency} (channel: {actual_channel_for_class}) sebagai target.\")\n                else:\n                    print(f\"[SALIENCY WARNING] Channel untuk kelas target ({actual_channel_for_class}) di luar jangkauan channel ({main_pred_tensor_from_raw.shape[1]}). Jatuh kembali ke skor objectness.\")\n                    if main_pred_tensor_from_raw.shape[1] > obj_score_channel_idx:\n                        target_score = main_pred_tensor_from_raw[0, obj_score_channel_idx, :, :].sum()\n                        print(f\"[SALIENCY DEBUG] Menggunakan jumlah skor objectness (channel: {obj_score_channel_idx}) sebagai target (fallback).\")\n                    else:\n                        print(f\"[SALIENCY ERROR] Channel objectness ({obj_score_channel_idx}) juga di luar jangkauan.\")\n                        target_score = None \n            elif main_pred_tensor_from_raw.shape[1] > obj_score_channel_idx: # Default jika tidak ada kelas target\n                target_score = main_pred_tensor_from_raw[0, obj_score_channel_idx, :, :].sum()\n                print(f\"[SALIENCY DEBUG] Menggunakan jumlah skor objectness (channel: {obj_score_channel_idx}) sebagai target (default).\")\n            \n            if target_score is not None:\n                 print(f\"[SALIENCY DEBUG] target_score (sebelum backward) dihitung. Nilai: {target_score.item()}\")\n            else:\n                print(f\"[SALIENCY ERROR] Gagal menghitung target_score dari main_pred_tensor_from_raw.\")\n        else:\n            print(\"[SALIENCY ERROR] Tidak dapat mengidentifikasi tensor prediksi utama (main_pred_tensor_from_raw is None).\")\n            if main_pred_tensor_from_raw is not None: del main_pred_tensor_from_raw\n            gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n            return None\n        \n        if main_pred_tensor_from_raw is not None: del main_pred_tensor_from_raw\n        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n        print(\"[SALIENCY DEBUG] Referensi main_pred_tensor_from_raw dihapus dan cache dikosongkan.\")\n\n        if target_score is None or (isinstance(target_score, torch.Tensor) and target_score.numel() == 0) :\n            print(\"[SALIENCY ERROR] target_score tidak valid atau kosong.\")\n            return None\n\n        print(\"[SALIENCY DEBUG] Akan melakukan zero_grad pada modul...\")\n        pytorch_module.zero_grad()\n        if input_image_tensor.grad is not None:\n            print(\"[SALIENCY DEBUG] Meng-nol-kan gradien input_image_tensor yang ada...\")\n            input_image_tensor.grad.zero_()\n        \n        print(\"[SALIENCY DEBUG] Akan melakukan target_score.backward()...\")\n        target_score.backward()\n        print(\"[SALIENCY DEBUG] target_score.backward() selesai.\")\n\n        del target_score\n        gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n        print(\"[SALIENCY DEBUG] target_score dihapus dan cache dikosongkan.\")\n\n        grad_tensor = input_image_tensor.grad # Simpan referensi ke gradien\n        if grad_tensor is None:\n            print(\"[SALIENCY ERROR] input_image_tensor.grad adalah None setelah backward().\")\n            return None\n        print(f\"[SALIENCY DEBUG] input_image_tensor.grad berhasil didapatkan. Shape: {grad_tensor.shape}, Device: {grad_tensor.device}\")\n\n        # --- BAGIAN YANG SEBELUMNYA MENYEBABKAN KERNEL MATI ---\n        # Kita akan memecah setiap operasi dan menambah try-except yang sangat spesifik\n        saliency_abs_grad = None\n        saliency_max_channels = None\n        saliency_squeezed = None\n        saliency_cpu = None\n        \n        try:\n            print(\"[SALIENCY DEBUG TRY-BLOCK] Memulai pemrosesan gradien...\")\n            print(\"[SALIENCY DEBUG STEP] Menghitung .abs() pada gradien...\")\n            saliency_abs_grad = grad_tensor.abs()\n            print(f\"[SALIENCY DEBUG STEP] .abs() selesai. Shape: {saliency_abs_grad.shape}, Device: {saliency_abs_grad.device}\")\n\n            print(\"[SALIENCY DEBUG STEP] Menghitung torch.max(..., dim=1)...\")\n            saliency_max_channels, _ = torch.max(saliency_abs_grad, dim=1)\n            print(f\"[SALIENCY DEBUG STEP] torch.max() selesai. Shape: {saliency_max_channels.shape}, Device: {saliency_max_channels.device}\")\n            del saliency_abs_grad; gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None # Hapus segera\n\n            print(\"[SALIENCY DEBUG STEP] Menghitung .squeeze(0)...\")\n            saliency_squeezed = saliency_max_channels.squeeze(0)\n            print(f\"[SALIENCY DEBUG STEP] .squeeze(0) selesai. Shape: {saliency_squeezed.shape}, Device: {saliency_squeezed.device}\")\n            del saliency_max_channels; gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\n            print(\"[SALIENCY DEBUG STEP] Memindahkan ke CPU (.cpu())...\")\n            saliency_cpu = saliency_squeezed.cpu()\n            print(f\"[SALIENCY DEBUG STEP] .cpu() selesai. Shape: {saliency_cpu.shape}, Device: {saliency_cpu.device}\")\n            del saliency_squeezed; gc.collect(); torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\n            print(\"[SALIENCY DEBUG STEP] Mengkonversi ke NumPy (.numpy())...\")\n            saliency_np = saliency_cpu.numpy()\n            print(\"[SALIENCY DEBUG STEP] .numpy() selesai. Saliency map (saliency_np) berhasil dibuat.\")\n            del saliency_cpu; gc.collect()\n\n        except RuntimeError as e_runtime: # Menangkap error PyTorch yang umum\n            print(f\"[SALIENCY ERROR RUNTIME] Saat memproses gradien: {e_runtime}\")\n            traceback.print_exc()\n            return None\n        except Exception as e_general: # Menangkap error Python lainnya\n            print(f\"[SALIENCY ERROR GENERAL] Saat memproses gradien: {e_general}\")\n            traceback.print_exc()\n            # Jika kernel mati karena error level C atau CUDA, ini mungkin tidak tertangkap.\n            return None\n\n        if saliency_np is None:\n             print(\"[SALIENCY ERROR] saliency_np adalah None sebelum normalisasi (kemungkinan error di atas).\")\n             return None\n\n        print(\"[SALIENCY DEBUG] Memulai normalisasi saliency_np...\")\n        saliency_min = saliency_np.min()\n        saliency_max = saliency_np.max()\n        if saliency_max - saliency_min > 1e-8:\n            saliency_np = (saliency_np - saliency_min) / (saliency_max - saliency_min)\n        else:\n            saliency_np = np.zeros_like(saliency_np)\n        saliency_np = np.uint8(saliency_np * 255)\n        print(\"[SALIENCY DEBUG] Saliency map dinormalisasi.\")\n        \n\n        return saliency_np\n\n    except Exception as e_main: # Error utama dalam fungsi\n        print(f\"[SALIENCY ERROR UTAMA] Terjadi error di generate_saliency_map: {e_main}\")\n        traceback.print_exc()\n        return None\n    finally:\n        # Pembersihan akhir\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        print(\"[SALIENCY DEBUG] Pembersihan akhir di generate_saliency_map selesai.\")\n\n# ... (sisa skrip __main__ Anda)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n# import matplotlib.pyplot as plt # Jika ingin menampilkan dengan matplotlib\n# from IPython.display import Image as IPImage, display # Jika ingin menampilkan dengan IPython\n\ndef visualize_saliency(original_image, saliency_map, detection_results=None,\n                       model_input_size=(640, 640), # Tambahkan argumen untuk ukuran input model\n                       alpha=0.5, output_path_prefix=\"saliency_output\"):\n    \"\"\"\n    Memvisualisasikan saliency map, menggambar bounding box yang diskalakan, dan menyimpannya ke file.\n    Args:\n        original_image (PIL.Image): Gambar asli (objek PIL Image).\n        saliency_map (numpy.ndarray): Saliency map (grayscale, uint8, 0-255).\n        detection_results (ultralytics.engine.results.Results, optional): Hasil deteksi dari YOLO.\n        model_input_size (tuple, optional): Ukuran (lebar, tinggi) gambar saat dimasukkan ke model.\n        alpha (float, optional): Transparansi heatmap.\n        output_path_prefix (str, optional): Prefix untuk nama file output.\n    \"\"\"\n    if saliency_map is None:\n        print(\"[VISUALIZE] Saliency map tidak tersedia untuk visualisasi.\")\n        return\n\n    print(f\"[VISUALIZE] Tipe data saliency_map: {saliency_map.dtype}, Shape: {saliency_map.shape}\")\n    if saliency_map.dtype != np.uint8:\n        print(\"[VISUALIZE] PERINGATAN: Tipe data saliency_map bukan np.uint8.\")\n        # Coba konversi (asumsi sudah dinormalisasi 0-1 jika float, atau perlu cast langsung)\n        if saliency_map.max() <= 1.0 and saliency_map.min() >= 0.0 and saliency_map.dtype != np.uint8:\n             saliency_map = np.uint8(saliency_map * 255)\n        else:\n             saliency_map = saliency_map.astype(np.uint8)\n        print(f\"[VISUALIZE] Saliency map dikonversi/di-cast ke uint8. Shape baru: {saliency_map.shape}\")\n\n    if len(saliency_map.shape) != 2: # Pastikan 2D\n        print(f\"[VISUALIZE] PERINGATAN: Saliency map shape {saliency_map.shape} bukan 2D.\")\n        if len(saliency_map.shape) == 3 and saliency_map.shape[2] == 1: saliency_map = saliency_map.squeeze()\n        elif len(saliency_map.shape) == 3: saliency_map = cv2.cvtColor(saliency_map, cv2.COLOR_BGR2GRAY) # Jika berwarna\n        else: print(\"[VISUALIZE] ERROR: Tidak bisa mengubah saliency_map menjadi 2D.\"); return\n        print(f\"[VISUALIZE] Shape saliency_map setelah penyesuaian: {saliency_map.shape}\")\n\n    try:\n        pil_original_width = original_image.width\n        pil_original_height = original_image.height\n        print(f\"[VISUALIZE] Ukuran gambar asli (PIL): Width={pil_original_width}, Height={pil_original_height}\")\n\n        saliency_map_resized = cv2.resize(saliency_map, (pil_original_width, pil_original_height))\n        print(f\"[VISUALIZE] Saliency map diubah ukurannya menjadi: {saliency_map_resized.shape}\")\n\n        heatmap = cv2.applyColorMap(saliency_map_resized, cv2.COLORMAP_JET)\n        print(f\"[VISUALIZE] Heatmap dibuat. Shape: {heatmap.shape}\")\n\n        original_cv_image = cv2.cvtColor(np.array(original_image), cv2.COLOR_RGB2BGR)\n        print(f\"[VISUALIZE] Gambar asli dikonversi ke OpenCV BGR. Shape: {original_cv_image.shape}\")\n\n        overlaid_image = cv2.addWeighted(original_cv_image, 1 - alpha, heatmap, alpha, 0)\n        print(f\"[VISUALIZE] Heatmap di-overlay ke gambar asli. Shape: {overlaid_image.shape}\")\n\n        # Gambar bounding box jika ada hasil deteksi\n        if detection_results and hasattr(detection_results, 'boxes') and len(detection_results.boxes) > 0:\n            model_input_width, model_input_height = model_input_size\n            \n            # Hitung faktor skala\n            scale_x = pil_original_width / model_input_width\n            scale_y = pil_original_height / model_input_height\n            print(f\"[VISUALIZE] Menggambar {len(detection_results.boxes)} bounding box dengan skala x:{scale_x:.2f}, y:{scale_y:.2f}\")\n\n            names = detection_results.names if hasattr(detection_results, 'names') else {int(c): f\"class_{int(c)}\" for c in detection_results.boxes.cls.unique()}\n\n            for i in range(len(detection_results.boxes)):\n                box_data = detection_results.boxes[i]\n                # Koordinat xyxy dari model berada dalam ruang input model (misal, 640x640)\n                xyxy_model_space = box_data.xyxy.squeeze().cpu().numpy()\n                conf = box_data.conf.squeeze().cpu().numpy()\n                cls_id = int(box_data.cls.squeeze().cpu().numpy())\n\n                # Skalakan koordinat ke ruang gambar asli\n                orig_x1_scaled = xyxy_model_space[0] * scale_x\n                orig_y1_scaled = xyxy_model_space[1] * scale_y\n                orig_x2_scaled = xyxy_model_space[2] * scale_x\n                orig_y2_scaled = xyxy_model_space[3] * scale_y\n                \n                # Konversi ke integer dan klip ke batas gambar asli\n                x1 = max(0, min(int(orig_x1_scaled), pil_original_width - 1))\n                y1 = max(0, min(int(orig_y1_scaled), pil_original_height - 1))\n                x2 = max(0, min(int(orig_x2_scaled), pil_original_width - 1))\n                y2 = max(0, min(int(orig_y2_scaled), pil_original_height - 1))\n                \n                label = f\"{names.get(cls_id, f'class_{cls_id}')}: {conf:.2f}\"\n\n                cv2.rectangle(overlaid_image, (x1, y1), (x2, y2), (0, 255, 0), 2) # Warna hijau (BGR)\n                cv2.putText(overlaid_image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n            print(\"[VISUALIZE] Bounding box selesai digambar dengan koordinat yang diskalakan.\")\n        else:\n            print(\"[VISUALIZE] Tidak ada hasil deteksi valid untuk digambar bounding boxnya.\")\n\n        original_filename = f\"{output_path_prefix}_original.png\"\n        heatmap_filename = f\"{output_path_prefix}_heatmap.png\"\n        overlaid_filename = f\"{output_path_prefix}_overlaid.png\"\n\n        cv2.imwrite(original_filename, original_cv_image)\n        print(f\"[VISUALIZE] Gambar asli disimpan ke: {original_filename}\")\n        cv2.imwrite(heatmap_filename, heatmap)\n        print(f\"[VISUALIZE] Heatmap disimpan ke: {heatmap_filename}\")\n        cv2.imwrite(overlaid_filename, overlaid_image)\n        print(f\"[VISUALIZE] Gambar overlaid disimpan ke: {overlaid_filename}\")\n        print(\"[VISUALIZE] Semua gambar berhasil disimpan.\")\n\n        # (Opsional: kode untuk menampilkan dengan matplotlib jika di notebook)\n        # import matplotlib.pyplot as plt\n        # fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n        # ax.imshow(cv2.cvtColor(overlaid_image, cv2.COLOR_BGR2RGB))\n        # ax.set_title(\"Overlaid Image with Scaled BBoxes\")\n        # ax.axis('off')\n        # plt.show()\n\n\n    except Exception as e:\n        print(f\"[VISUALIZE] Terjadi error dalam fungsi visualize_saliency: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL_INPUT_WIDTH = 640\nMODEL_INPUT_HEIGHT = 640\n\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    IMAGE_PATH = '/kaggle/input/explainable/contoh-1.jpeg'\n\n    # Muat model YOLOv8 Anda (pastikan 'model' sudah didefinisikan sebagai objek YOLO)\n    # Contoh: model = YOLO('yolov8n.pt') atau model custom Anda\n    # Jika model Anda adalah model custom yang mendeteksi \"table\" sebagai kelas 0, pastikan itu yang dimuat.\n    # model = YOLO('path/to/your/custom_model_detecting_table.pt') # GANTI JIKA PERLU\n\n    try:\n        original_image, image_tensor = preprocess_image(IMAGE_PATH) # Fungsi preprocess_image Anda\n        print(f\"Image tensor shape awal: {image_tensor.shape}, device: {image_tensor.device}\")\n    except FileNotFoundError:\n        print(f\"Error: File gambar tidak ditemukan di {IMAGE_PATH}\")\n        exit()\n    except Exception as e:\n        print(f\"Error saat memproses gambar: {e}\")\n        exit()\n\n    # Dapatkan hasil deteksi aktual dari model.predict() untuk visualisasi nanti\n    # dan untuk mengetahui apa yang sebenarnya dideteksi.\n    print(\"\\nMemeriksa hasil prediksi model secara langsung (untuk visualisasi)...\")\n    detection_results_for_visualization = None\n    try:\n        # Pastikan 'model' adalah objek YOLO yang sudah dimuat\n        raw_results_list = model.predict(image_tensor, verbose=False) # verbose=False agar tidak terlalu ramai\n        if raw_results_list and len(raw_results_list) > 0:\n            detection_results_for_visualization = raw_results_list[0] # Ambil objek Results pertama\n            num_detections = len(detection_results_for_visualization.boxes)\n            print(f\"Model mendeteksi {num_detections} objek untuk visualisasi.\")\n            if num_detections > 0:\n                 print(f\"  Contoh deteksi: conf={detection_results_for_visualization.boxes.conf[0].item():.2f}, cls={int(detection_results_for_visualization.boxes.cls[0].item())}\")\n        else:\n            print(\"Model tidak memberikan hasil prediksi untuk visualisasi.\")\n    except Exception as e:\n        print(f\"Error saat menjalankan model.predict() untuk visualisasi: {e}\")\n\n    # --- Generate saliency map ---\n    # Kita tahu kelas 0 (\"table\") terdeteksi. Mari kita coba jelaskan kelas ini.\n    # Panggil dengan pytorch_module (model.model) dan image_tensor\n    # Fungsi generate_saliency_map akan menangani pemindahan tensor ke device yang benar\n    print(\"\\nMemulai pembuatan saliency map...\")\n    saliency_map_np = generate_saliency_map(\n        pytorch_module=model.model,  # Ini adalah nn.Module\n        input_image_tensor=image_tensor,\n        target_class_idx_for_saliency=0  # Jelaskan kelas 0 (\"table\")\n    )\n\n    if saliency_map_np is not None:\n        print(\"Saliency map berhasil dibuat.\")\n        visualize_saliency(\n            original_image, # Objek PIL Image asli\n            saliency_map_np,\n            detection_results_for_visualization, # Hasil dari model.predict()\n            model_input_size=(MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT) # <-- Berikan ukuran ini\n        )\n    else:\n        print(\"Gagal membuat saliency map.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}